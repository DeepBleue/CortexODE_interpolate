{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import trimesh\n",
    "import matplotlib.pyplot as plt\n",
    "from util.make_vtk import write_lines_to_vtk,write_points_to_vtk\n",
    "from util.vtk_revise import read_vtk,write_vtk\n",
    "from util.generate_interpolate_vector import generate_multiple_vectors\n",
    "from util.util import normalize_points\n",
    "import pyautogui\n",
    "\n",
    "\n",
    "\n",
    "def get_thick_norm_and_ori(file_name,white,pial,separate_num):\n",
    "    white_vertices = white['vertices']\n",
    "    white_faces = white['faces'][:,1:]\n",
    "\n",
    "    pial_vertices = pial['vertices']\n",
    "    pial_faces = pial['faces'][:,1:]\n",
    "\n",
    "    threshold = max(white['thickness'])\n",
    "\n",
    "    origin_mesh = trimesh.Trimesh(vertices=white_vertices, faces=white_faces)\n",
    "    target_mesh = trimesh.Trimesh(vertices=pial_vertices, faces=pial_faces)\n",
    "\n",
    "    origin_directions = origin_mesh.vertex_normals.copy()\n",
    "    white2pial_dir = normalize_points(pial_vertices - white_vertices)\n",
    "\n",
    "    dir_sign = (np.sum(white2pial_dir*origin_directions,axis=1) < 0)\n",
    "    origin_directions[dir_sign] *= -1\n",
    "\n",
    "\n",
    "    all_origins = []\n",
    "\n",
    "    print('--- Generate Multiple Vectors ---')\n",
    "\n",
    "    all_directions = generate_multiple_vectors(origin_directions,white2pial_dir , separate_num)\n",
    "    for idx in range(len(white_vertices)):\n",
    "        origin_pos = white_vertices[idx]\n",
    "        all_origins.extend([origin_pos] * separate_num)\n",
    "\n",
    "    print('--- Search intersect location ---')\n",
    "    locations, index_ray, index_tri = target_mesh.ray.intersects_location(\n",
    "        ray_origins=all_origins, \n",
    "        ray_directions=all_directions)\n",
    "    \n",
    "    pairs = []\n",
    "    thicknesses = []\n",
    "    not_intersect = []\n",
    "    not_intersect_idx = []\n",
    "    too_long_pair = []\n",
    "\n",
    "    index_groups = {}\n",
    "\n",
    "    print('--- Data setting ---')\n",
    "\n",
    "    for i, ray_idx in tqdm(enumerate(index_ray), total=len(index_ray)):\n",
    "        if ray_idx not in index_groups:\n",
    "            index_groups[ray_idx] = []\n",
    "        index_groups[ray_idx].append(i)\n",
    "\n",
    "\n",
    "\n",
    "    print('--- Calculating ---')\n",
    "    \n",
    "    for idx in tqdm(range(0, len(all_origins), separate_num)):\n",
    "\n",
    "        real_idx_2 = []\n",
    "        for i in range(idx, idx+separate_num):\n",
    "            real_idx_2.extend(index_groups.get(i, []))\n",
    "            \n",
    "        subset_locations = locations[real_idx_2]\n",
    "\n",
    "        subset_origin = all_origins[idx]\n",
    "        \n",
    "        distances = [np.linalg.norm(loc - subset_origin) for loc in subset_locations]\n",
    "\n",
    "        if distances:\n",
    "            min_distance_index = np.argmin(distances)\n",
    "            closest_location = subset_locations[min_distance_index]\n",
    "            closest_distance = distances[min_distance_index]\n",
    "            \n",
    "            \n",
    "            if closest_distance > threshold:\n",
    "                too_long_pair.append((subset_origin, closest_location))\n",
    "                not_intersect.append(subset_origin)\n",
    "                real_idx = int(idx/separate_num)\n",
    "                not_intersect_idx.append(real_idx)\n",
    "                white2pial_dist = np.linalg.norm(white_vertices[real_idx]-pial_vertices[real_idx])\n",
    "                pairs.append((white_vertices[real_idx], pial_vertices[real_idx]))\n",
    "                thicknesses.append(white2pial_dist)\n",
    "\n",
    "                \n",
    "            else : \n",
    "                pairs.append((subset_origin, closest_location))\n",
    "                thicknesses.append(closest_distance)\n",
    "        else:\n",
    "            not_intersect.append(subset_origin)\n",
    "            not_intersect_idx.append(int(idx/50))\n",
    "            \n",
    "            real_idx = int(idx/50)\n",
    "            not_intersect_idx.append(real_idx)\n",
    "            white2pial_dist = np.linalg.norm(white_vertices[real_idx]-pial_vertices[real_idx])\n",
    "            pairs.append((white_vertices[real_idx], pial_vertices[real_idx]))\n",
    "            thicknesses.append(white2pial_dist)\n",
    "\n",
    "    nan_count = np.isnan(thicknesses).sum()\n",
    "    thicknesses = np.array(thicknesses)\n",
    "    white['new_thickness'] = thicknesses\n",
    "    not_inter_ori_ray = []\n",
    "    for i in not_intersect_idx : \n",
    "        not_inter_ori_ray.append((white_vertices[i],pial_vertices[i]))\n",
    "\n",
    "\n",
    "    print(f'num Nan : {nan_count}')\n",
    "    print(f'not intersect : {len(not_intersect)}')\n",
    "    print(f'Max thickness : {max(thicknesses)}')\n",
    "    print(f'min thickness : {min(thicknesses)}')\n",
    "\n",
    "    write_vtk(white,f'./output/white_data/' + file_name + \".vtk\")\n",
    "    write_lines_to_vtk(pairs, f\"./output/pair_line/\"+file_name+\"pair_line.vtk\")\n",
    "    write_points_to_vtk(not_intersect,f\"./output/not_intersect_point/\"+file_name+\"not_intersect_point.vtk\")\n",
    "    write_lines_to_vtk(too_long_pair, f\"./output/long_line/\"+file_name+\"long_line.vtk\")\n",
    "    write_lines_to_vtk(not_inter_ori_ray, f\"./output/not_inter_ori_ray/\"+file_name+\"not_inter_ori_ray.vtk\")\n",
    "    pyautogui.press('enter')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from util.vtk_revise import read_vtk\n",
    "# from get_thickness_with_interpolate import get_thick_norm_and_ori\n",
    "\n",
    "\n",
    "\n",
    "rootDir = './AllCortexData'\n",
    "count = 0 \n",
    "\n",
    "for dirName, subdirList, fileList in os.walk(rootDir):\n",
    "    \n",
    "    \n",
    "    \n",
    "    read_dir_name = dirName.replace(\"\\\\\", \"/\")\n",
    "    if 'SUBJ' in dirName : \n",
    "        subject_num = re.findall(r'\\d+', read_dir_name)[0]\n",
    "        \n",
    "    full_file_name_lh_white = None\n",
    "    full_file_name_lh_pial = None\n",
    "    full_file_name_rh_white = None\n",
    "    full_file_name_rh_pial  = None\n",
    "    \n",
    "    for fname in fileList:\n",
    "        if 'vtk' in fname:\n",
    "            if 'lh' in fname : \n",
    "                if 'white' in fname : \n",
    "                    full_file_name_lh_white = read_dir_name + '/' + fname\n",
    "                    \n",
    "                elif 'pial' in fname : \n",
    "                    full_file_name_lh_pial = read_dir_name + '/' + fname\n",
    "                    \n",
    "                \n",
    "                \n",
    "            elif 'rh' in fname : \n",
    "                if 'white' in fname : \n",
    "                    full_file_name_rh_white = read_dir_name + '/' + fname\n",
    "                elif 'pial' in fname : \n",
    "                    full_file_name_rh_pial = read_dir_name + '/' + fname\n",
    "\n",
    "    \n",
    "    if full_file_name_lh_white != None and \\\n",
    "        full_file_name_lh_pial != None and \\\n",
    "        full_file_name_rh_white != None and \\\n",
    "        full_file_name_rh_pial != None : \n",
    "            \n",
    "        ### lh brain ###\n",
    "        print(f\"-----------------SUBJECT {subject_num} LH BRAIN START----------------\")\n",
    "        lh_white = read_vtk(full_file_name_lh_white) \n",
    "        lh_pial = read_vtk(full_file_name_lh_pial)  \n",
    "        \n",
    "        if 'BL' in full_file_name_lh_white :\n",
    "            lh_file_name = 'SUBJ_' +  subject_num + '_lh_white_BL'\n",
    "\n",
    "        elif 'FU' in full_file_name_lh_white :\n",
    "            lh_file_name = 'SUBJ_' +  subject_num + '_lh_white_FU'\n",
    "        \n",
    "        get_thick_norm_and_ori(lh_file_name,lh_white,lh_pial,50)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        ### rh brain ###\n",
    "        print(f\"-----------------SUBJECT {subject_num} RH BRAIN START----------------\")\n",
    "        rh_white = read_vtk(full_file_name_rh_white)  \n",
    "        rh_pial = read_vtk(full_file_name_rh_pial)  \n",
    "\n",
    "        if 'BL' in full_file_name_rh_white :\n",
    "            rh_file_name = 'SUBJ_' +  subject_num + '_rh_white_BL'\n",
    "\n",
    "        elif 'FU' in full_file_name_rh_white :\n",
    "            rh_file_name = 'SUBJ_' +  subject_num + '_rh_white_FU'\n",
    "\n",
    "        get_thick_norm_and_ori(rh_file_name,rh_white,rh_pial,50)\n",
    "\n",
    "\n",
    "\n",
    "               \n",
    "\n",
    "print('#####------- ALL PROSCESS DONE -------#####')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Name  Age      City\n",
      "0  John   28  New York\n",
      "1  Anna   22    London\n",
      "2  Mike   32    Sydney\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Name': ['John', 'Anna', 'Mike'],\n",
    "    'Age': [28, 22, 32],\n",
    "    'City': ['New York', 'London', 'Sydney']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('output.csv')\n",
    "\n",
    "filename = \"output.csv\"\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    # If the file doesn't exist, create an empty DataFrame and save it as \"output.csv\"\n",
    "    df_empty = pd.DataFrame()\n",
    "    df_empty.to_csv(filename)\n",
    "    print(f\"'{filename}' created!\")\n",
    "else:\n",
    "    # If the file exists, read its content\n",
    "    df = pd.read_csv(filename)\n",
    "    print(f\"'{filename}' read successfully!\")\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1205908473.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(np.mean(a).2f)\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "a = np.array([1,2,4])\n",
    "\n",
    "print(np.mean(a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CortexODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
